<!DOCTYPE html>
<html>

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q75H4YSQ8M"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-Q75H4YSQ8M');
    </script>
    <!-- Adapted from Dave Epstein's BlogGAN webpage template: https://dave.ml/blobgan/ -->
    <title>Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models
    </title>
    <link rel="stylesheet" href="./bulma.min.css" />
    <!-- <link rel="icon" type="image/x-icon" href="/images/logo_ai.png"> -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <style>
        html,
        body,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6 {
            font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        }

        .cite {
            padding: 0px;
            background: #ffffff;
            font-size: 18px
        }

        .card {
            border: 1px solid #ccc
        }

        /* img { margin-bottom:-6px;} */
        p {
            font-size: 18px;
        }
    </style>
    <meta charset="UTF-8">
    <meta name="description"
        content="Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models">
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title></title>
    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="manifest" href="site.webmanifest">
    <meta name="theme-color" content="#f5ece5">
    <link rel="mask-icon" href="safari-pinned-tab.svg" color="#276FBF">
    <meta name="msapplication-TileColor" content="#276FBF">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <style>
        html,
        body {
            font-family: 'GT Ultra', sans-serif !important;
            font-weight: 200;
            font-size: 18px;
            overflow-x: hidden;
            box-sizing: border-box;
            color: #1f1e1d;
            background-color: #f5ece5;
        }

        *,
        *:before,
        *:after {
            box-sizing: inherit;
        }

        header {
            text-align: center;
        }

        a {
            /* color: #276FBF !important; */
            color: #276FBF;
            text-decoration: none;
            font-weight: 300;
        }

        a:hover {
            text-decoration: underline;
            /* color: #184477 !important; */
            color: #184477;
        }

        section h3 {
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
        }

        section h5,
        section h3 {
            justify-content: space-between;

            display: flex;
            align-items: center;
        }
    </style>

    <style>
        .outer {
            width: 100%;
            height: 200px;
            background-color: red;
        }

        .inner {
            position: relative;
            width: 100%;
            text-align: center;
            margin: auto;
            color: white;
            cursor: crosshair;
            user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            -webkit-user-select: none;
        }

        .log {
            position: relative;
            width: 100%;
            text-align: center;
        }

        .inner_cat {
            position: relative;
            width: 100%;
            text-align: center;
            margin: auto;
            color: white;
            cursor: crosshair;
            user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            -webkit-user-select: none;
        }

        .log_cat {
            position: relative;
            width: 100%;
            text-align: center;
        }

        .inner_sn {
            position: relative;
            width: 100%;
            text-align: center;
            margin: auto;
            color: white;
            cursor: crosshair;
            user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            -webkit-user-select: none;
        }

        .log_sn {
            position: relative;
            width: 100%;
            text-align: center;
        }

        .inner_gc {
            position: relative;
            width: 100%;
            text-align: center;
            margin: auto;
            color: white;
            cursor: crosshair;
            user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            -webkit-user-select: none;
        }

        .log_gc {
            position: relative;
            width: 100%;
            text-align: center;
        }

        .inner_shading {
            position: relative;
            width: 100%;
            text-align: center;
            margin: auto;
            color: white;
            cursor: crosshair;
            user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            -webkit-user-select: none;
        }

        .log_shading {
            position: relative;
            width: 100%;
            text-align: center;
        }

        .inner_gqa {
            position: relative;
            width: 100%;
            text-align: center;
            margin: auto;
            color: white;
            cursor: crosshair;
            user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            -webkit-user-select: none;
        }

        .log_gqa {
            position: relative;
            width: 100%;
            text-align: center;
        }

        .inner_demo {
            position: relative;
            width: 100%;
            text-align: center;
            margin: auto;
            color: white;
            cursor: crosshair;
            user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            -webkit-user-select: none;
        }

        .log_demo {
            position: relative;
            width: 100%;
            text-align: center;
        }

        /* .demo_gif {
        position: relative;
        width: 50%;
        text-align: center;
        margin: auto;
        } */
    </style>


    <style>
        .project-links {
            /* width: 400px;
        height: 400px;
        position: absolute;
        left: 50%;
        top: 50%;
        transform: translate(-50%, -50%);
        display: flex;
        justify-content: center; */
            max-width: 600px;
            margin: 0 auto;
            align-items: center;
            display: flex;
            gap: 20px;
            justify-content: center;
            flex-wrap: wrap;
        }
    </style>

    <!-- Paper button -->
    <style>
        .btn-paper {
            width: 95px;
            height: 35px;
            cursor: pointer;
            border-radius: 25px;
            background: transparent;
            border: 2px solid #91C9FF;
            outline: none;
            transition: 1s ease-in-out;
            text-decoration: none;
            align-items: center;
            display: flex;
            text-align: center;
            font-weight: 500;
            color: black;
        }

        .btn-paper:hover {
            transition: 0.75s ease-in-out;
            background: #8bb2ff1e;
            text-decoration: none;
            font-weight: 500;
            color: black;
        }

        .vertical-center-paper {
            height: 80%;
            position: relative;
            /* top: 0px;
            right: 0;
            float: left; */
            padding-right: 1px;
            padding-left: 6px;
        }
    </style>

    <!-- Code button -->
    <style>
        .btn-code {
            width: 95px;
            height: 35px;
            cursor: pointer;
            border-radius: 25px;
            background: transparent;
            border: 2px solid #91C9FF;
            outline: none;
            transition: 1s ease-in-out;
            text-decoration: none;
            align-items: center;
            display: flex;
            text-align: center;
            font-weight: 500;
            color: black;
        }

        .btn-code:hover {
            transition: 0.75s ease-in-out;
            background: #8bb2ff1e;
            text-decoration: none;
            font-weight: 500;
        }

        .vertical-center-code {
            height: 60%;
            position: relative;
            /* top: 0px;
            left: 0px;
            float: left; */
            padding-right: 8px;
            padding-left: 10px;
        }
    </style>

    <!-- Citation button -->
    <style>
        .btn-citation {
            width: 110px;
            height: 35px;
            cursor: pointer;
            border-radius: 25px;
            background: transparent;
            border: 2px solid #91C9FF;
            outline: none;
            transition: 1s ease-in-out;
            text-decoration: none;
            align-items: center;
            display: flex;
            text-align: center;
            font-weight: 500;
            color: black;
        }

        .btn-citation:link {
            color: black;
        }

        .btn-citation:visited {
            color: black;
        }

        /* .btn-citation:hover {
            color: black;
        } */
        .btn-citation:active {
            color: black;
        }

        .btn-citation:hover {
            transition: 0.75s ease-in-out;
            background: #8bb2ff1e;
            text-decoration: none;
            font-weight: 500;
            color: black;
        }

        .vertical-center-cit {
            height: 70%;
            position: relative;
            /* top: 0px;
            right: 0;
            float: left; */
            padding-right: 7px;
            padding-left: 6px;
        }
    </style>

    <!-- Interactive button -->
    <style>
        .btn-interact {
            width: 133px;
            height: 35px;
            cursor: pointer;
            border-radius: 25px;
            background: transparent;
            border: 2px solid #91C9FF;
            outline: none;
            transition: 1s ease-in-out;
            text-decoration: none;
            align-items: center;
            display: flex;
            text-align: center;
            font-weight: 500;
            color: black;
        }

        .btn-interact:link {
            color: black;
        }

        .btn-interact:visited {
            color: black;
        }

        /* .btn-citation:hover {
                color: black;
            } */
        .btn-interact:active {
            color: black;
        }

        .btn-interact:hover {
            transition: 0.75s ease-in-out;
            background: #8bb2ff1e;
            text-decoration: none;
            font-weight: 500;
            color: black;
        }

        .vertical-center-interact {
            height: 70%;
            position: relative;
            /* top: 0px;
                right: 0;
                float: left; */
            padding-right: 7px;
            padding-left: 6px;
        }
    </style>

    <style>
        .vertical-center {
            height: 80%;
            position: relative;
            top: 0px;
            right: 0;
            float: left;
        }
    </style>

    <style>
        /* The grid: Four equal columns that floats next to each other */
        .column {
            float: left;
            width: 25% !important;
            padding: 10px;
        }

        /* Style the images inside the grid */
        .column img {
            opacity: 0.8;
            cursor: pointer;
        }

        .column img:hover {
            opacity: 1;
        }

        /* Clear floats after the columns */
        .row:after {
            content: "";
            display: table;
            clear: both;
        }

        /* The expanding image container (positioning is needed to position the close button and the text) */
        .container {
            position: relative;
            display: none;
        }

        /* Expanding image text */
        #imgtext {
            position: absolute;
            bottom: 15px;
            left: 15px;
            color: white;
            font-size: 20px;
        }

        /* Closable button inside the image */
        .closebtn {
            position: absolute;
            top: 10px;
            right: 15px;
            color: rgb(0, 0, 0);
            font-size: 35px;
            cursor: pointer;
        }

        .centered {
            position: relative;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: rgb(0, 0, 0);
            background-color: rgba(0, 0, 0, 0.26);
        }

        .text-block {
            position: absolute;
            /* bottom: 25px;
        right: 20px; */
            transform: translate(-50%, -50%);
            top: 50%;
            left: 50%;
            background-color: rgba(0, 0, 0, 0.26);
            color: white;
            padding-left: 30px;
            padding-right: 30px;
        }

        .parentContainer {
            position: relative;
            text-align: center;
            color: white;
        }

        .container {
        display: flex;
        }

        .column-left,
        .column-right {
        width: 50%;
        }

        .video-column {
            margin-left: -10px;
            margin-right: -10px;
            margin-bottom: -15px;
            padding-bottom: 0px;
        }

        .video-column .column {
            padding: 0 5px;
        }
    </style>
</head>

<body class="w3-white">
    <!-- Page Container -->
    <div class="w3-content w3-margin-top w3-margin-bottom" style="max-width:960px;">

        <!-- The Grid -->
        <div class="w3-row-padding">

            <!-- paper container -->
            <div class="w3-display-container w3-row w3-white w3-margin-bottom">
                <div class="w3-center">
                    <h2>Open-Ended Instructable Embodied Agents with
                    Memory-Augmented Large Language Models</h2>
                    <h5><a href="https://www.gabesarch.me">Gabriel Sarch</a> &emsp;&emsp; 
                        <a href="https://www.yuewu.ml/">Yue Wu</a> &emsp;&emsp;
                        <a
                            href="https://sites.google.com/andrew.cmu.edu/tarrlab/people/michael-j-tarr">Michael
                            Tarr</a> &emsp;&emsp;
                        <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a> 
                    </h5>
                    <div class="w3-center">
                        <h5>Carnegie Mellon University</h5>
                    </div>
                </div>

                <br>

                <div class="project-links">
                    <a href="" target="_blank" class="btn-paper"
                        role="button">
                        <img src="images/paper.png" class="vertical-center-paper"> Paper
                    </a>
                    <a href="https://github.com/Gabesarch/HELPER" target="_blank" class="btn-code"
                        role="button">
                        <img src="images/code.png" class="vertical-center-code"> Code
                    </a>
                    <!-- <a href="#results" class="btn-interact" role="button">
                        <img src="images/brain-clipart-lg.png" class="vertical-center-interact"> Interactive
                    </a> -->
                    <!-- <a href="#citation" class="btn-citation" role="button">
                        <img src="images/googlescholar.png" class="vertical-center-cit"> Citation
                    </a> -->
                </div>

                <br>
                <spacer type="horizontal" width="5" height="5"> </spacer>
                <br>
                
                <!-- Taken from view-source:https://voyager.minedojo.org/ -->
                <section class="section">
                    <div class="container is-max-widescreen">
                        <div class="rows">
                            <div class="rows is-centered">
                                <div class="row is-full-width">
                                    <div class="columns">
                                        <div class="column has-text-left video-column">
                                            <video controls muted autoplay loop width="100%">
                                                <source src="images/task_videos_new/Example_Video2.m4v" type="video/mp4" />
                                            </video>
                                        </div>
                
                                        <div class="column has-text-left video-column">
                                            <video controls muted autoplay loop width="100%">
                                                <source src="images/task_videos_new/Example_Video1_Clean.m4v" type="video/mp4" />
                                            </video>
                                        </div>
                
                                        <div class="column has-text-left video-column">
                                            <video controls muted autoplay loop width="100%">
                                                <source src="images/task_videos_new/Example_Video3.m4v" type="video/mp4" />
                                            </video>
                                        </div>
                                    </div>
                                    <div class="columns">
                                        <div class="column has-text-left video-column">
                                            <video controls muted autoplay loop width="100%">
                                                <source src="images/task_videos_new/Example_Video4.m4v" type="video/mp4" />
                                            </video>
                                        </div>
                
                                        <div class="column has-text-left video-column">
                                            <video controls muted autoplay loop width="100%">
                                                <source src="images/task_videos_new/Example_Video6.m4v" type="video/mp4" />
                                            </video>
                                        </div>
                
                                        <div class="column has-text-left video-column">
                                            <video controls muted autoplay loop width="100%">
                                                <source src="images/task_videos_new/Example_Video5.m4v" type="video/mp4" />
                                            </video>
                                        </div>
                                    </div>
                                    <!-- <div class="columns">
                                        <div class="column has-text-left video-column">
                                            <video controls muted autoplay loop width="100%">
                                                <source src="assets/videos/gallery/portal.mp4" type="video/mp4" />
                                            </video>
                                        </div>
                
                                        <div class="column has-text-left video-column">
                                            <video controls muted autoplay loop width="100%">
                                                <source src="assets/videos/gallery/pig.mp4" type="video/mp4" />
                                            </video>
                                        </div>
                
                                        <div class="column has-text-left video-column">
                                            <video controls muted autoplay loop width="100%">
                                                <source src="assets/videos/gallery/fish.mp4" type="video/mp4" />
                                            </video>
                                        </div>
                                    </div> -->
                                </div>
                            </div>
                        </div>
                    </div>
                </section>
                
                

                
                <!-- <div class="container">
                    <div class="column-left">Water plant</div>
                    <div class="column-right">Prepare coffee in a clean mug</div>
                </div>
                <div id="wrapper">
                    <video id="home1" width="49%" poster="images/video.jpg" controls="controls" preload="none">
                        <source type="video/mp4"
                            src="images/task_videos/612f99763b6978b6_0c1a.tfd_612f99763b6978b6_0c1a_52_fcd65c4a22cb59a8e4ad.mp4" />
                    </video>
                    <video id="home2" width="49%" poster="images/video.jpg" controls="controls" preload="none">
                        <source type="video/mp4"
                            src="images/task_videos/7688a09fa6b789ca_6db5.tfd_7688a09fa6b789ca_6db5_61_db7943b97465ca20954f.mp4" />
                    </video>
                    <div class="clear"></div>
                </div>
                <div class="container">
                    <div class="column-left">Put all credit cards on the bed</div>
                    <div class="column-right">Put a slice of lettuce on a clean plate</div>
                </div>
                <div id="wrapper">
                    <video id="home1" width="49%" poster="images/video.jpg" controls="controls" preload="none">
                        <source type="video/mp4"
                            src="images/task_videos/8463e3c141964336_c9cc.tfd_8463e3c141964336_c9cc_69_7a9e16fc939574b527cb.mp4" />
                    </video>
                    <video id="home2" width="49%" poster="images/video.jpg" controls="controls" preload="none">
                        <source type="video/mp4"
                            src="images/task_videos/2f8d482d22e56fdd_3e5c.tfd_2f8d482d22e56fdd_3e5c_188_bb2f62c23329a80458cc.mp4" />
                    </video>
                    <div class="clear"></div>
                </div>
                <div class="container">
                    <div class="column-left">Clean all plates</div>
                    <div class="column-right">Put all sports equipment on the bed</div>
                </div>
                <div id="wrapper">
                    <video id="home1" width="49%" poster="images/video.jpg" controls="controls" preload="none">
                        <source type="video/mp4"
                            src="images/task_videos/823f0fe4bda8607c_ee5e.tfd_823f0fe4bda8607c_ee5e_285_89fd71070236d5d8630b.mp4" />
                    </video>
                    <video id="home2" width="49%" poster="images/video.jpg" controls="controls" preload="none">
                        <source type="video/mp4"
                            src="images/task_videos/f892b2976d0955e0_b0d9.tfd_f892b2976d0955e0_b0d9_329_f53d766de44f3fd422ec.mp4" />
                    </video>
                    <div class="clear"></div>
                </div> -->


                <!-- <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <img src="images/Example_each_room_fast.gif" style="width:100%">
                </div> -->
                
                <!-- <div class="w3-center">
                    <h2>Abstract</h2>
                </div> -->
                <!-- <p align="justify">
                    We introduce HELPER, an open-ended, em
                    bodied home assistant that grounds free-form
                    human-robot instructions, user feedback, per
                    sonalized plans, object search queries, and er
                    ror feedback to executable programs by query
                    ing foundational language and vision mod
                    els. At its core, HELPER maintains an ever-
                    expanding key-value memory of language and
                    corresponding program pairs, and retrieves
                    relevant examples from memory on-the-fly
                    for context-dependent prompting of a pre
                    trained Large Language Model (LLM) to gen
                    erate programs. When execution problems
                    arise, HELPER queries a pretrained Vision-
                    Language Model (VLM) to obtain a visually-
                    grounded failure reason, and obtains simi
                    lar error correction examples to prompt the
                    memory-augmented LLM for a corrective pro
                    gram. When an object-of-interest has not been
                    detected by the agent, HELPER searches for
                    the object based on commonsense location
                    proposals by the LLM. HELPER executes the
                    programs and keeps track of object states us
                    ing off-the-shelf perceptual modules and low-
                    level manipulation skills. HELPER expands
                    its language-program memory during user de
                    ployment to adapt its behaviour to a user’s
                    preferences. 
                </p> -->
                <!-- <br> -->
                <!-- <spacer type="horizontal" width="1" height="1"> </spacer> -->
                <!-- <br> -->
                <!-- <p align="justify">
                    We show HELPER can recall recently
                    demonstrated routines, as well as adapt them
                    to complex variations of the routines. HELPER
                    sets a new state-of-the-art in the TEACh Execu
                    tion from Dialog History (EDH) and Trajectory
                    from Dialogue (TfD) benchmarks, improving
                    task success by 1.7x and goal-condition suc-
                    cess by 2.1x over existing works with minimal
                    in-domain finetuning. HELPER can also ask
                    for and incorporate user feedback, achieving
                    an additional 1.3x task success improvement
                    when doing so. HELPER demonstrates the po
                    tential of memory-augmented LLM and VLM
                    prompting for semantic parsing of open-ended
                    free-form instructions and dialogues into an ex
                    pandable library of programs.
                </p> -->
                <!-- <hr> -->

                <!-- </script> -->
                <!-- <div class="outer"> -->
                <!-- <div class="inner">
                    <img src="images/median_abs_depths_subject1.png" id="brain_image1" style="width:100%">
                </div>
                <div class="log">
                    <p>
                        Offset-relative: <span id="offsetX">0</span>
                    </p>
                </div> -->

                <!-- </section> -->

                <br>
                <spacer type="horizontal" width="100" height="50"> </spacer>
                <br>
                <hr>
                <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <!-- <h2>What can HELPER do?</h2> -->
                    <h2>How does HELPER work?</h2>
                    <p align="justify">We enhance Large Language Models (LLMs) with an external memory storing language-program pairs for in-context example
                    retrieval during task plan generation. The model processes instructions, dialogues, corrections, and visual environment
                    descriptions, retrieving pertinent memories for context-aware predictions of task plans and their adjustments. Our agent
                    implements these plans through visual input, utilizing map building, 3D object detection, state tracking, and active
                    exploration guided by the LLM's common sense for object location. Successful programs, paired with their linguistic
                    context, are archived in memory, facilitating personalized future interactions.</p>
                    <div class="w3-center">
                        <video width="100%" playsinline autoplay muted loop>
                            <source src="images/Figure1_Video_Cropped.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>

                    
                    <!-- <p align="justify">
                        HELPER is an open-ended, embodied home assistant that grounds free-form human-robot instructions, user
                        feedback, personalized plans, object search queries, and error feedback to executable programs by querying
                        foundational language and vision models.
                    </p> -->
                    <br>
                    <!-- <p align="justify">
                        Inspired by <a href="https://www.biorxiv.org/content/10.1101/2022.03.16.484578v2"
                            target="_blank">Khosla & Wehbe 2022</a>, brain dissection provides a hypothesis-neutral
                        approach for identifying the most significant image features for
                        predicting the response of a specific voxel. </p> -->
                    <!-- <br> -->
                    <!-- <hr style="height:1px; visibility:hidden;" > -->
                    <!-- <p align="justify">The method trains a convolutional neural network tailored to predict voxel
                        responses to natural images within a defined sub-region. By training a backbone network for this
                        sub-region and incorporating a linear
                        readout for each voxel, the network learns to extract image features crucial for predicting each
                        voxel
                        (<b>Figure A below</b>). For training, we used the <a href="https://naturalscenesdataset.org/"
                            target="_blank">Natural Scenes Dataset (NSD)</a>, which consists of high-resolution
                        fMRI responses to naturalistic images from <a href="https://cocodataset.org/#home"
                            target="_blank">Microsoft COCO</a>.</p> -->
                    <!-- <br> -->
                    <!-- <hr style="height:3px; visibility:hidden;" /> -->
                    <!-- <p align="justify">After training, we "dissect" the network by inputting held-out images and
                        extracting regions that
                        the network considers most relevant for each voxel (<b>Figure B below</b>). Subsequently, we
                        analyze properties of the
                        voxel-selective regions within the images.
                    </p> -->
                    <h3 align="justify">Framework Overview</h3>
                    <p align="justify">Helper is an an embodied agent
                    equipped with as external memory of language-program pairs that parses free-form human-robot dialogue into action programs through
                    retrieval-augmented LLM prompting. The programs are then translated into specific
                    navigation and manipulation actions from RGB input.</p>
                    <p align="justify">Helper is made up of the following modules:</p>
                    <img src="images/Full_Pipeline_Figure_V4.jpg" style="width:60%">
                    <p align="justify">
                    <ol>
                    <li align="justify"><b>PLANNER</b>: The PLANNER obtains the most relevant task plans from memory to prompt
                    an LLM to transform user instructions, dialogue, failure reasons, or user feedback into an executable program.</li>
                    <li align="justify"><b>Plan correction</b>: When an action fails, a pretrained vision-language model infers the failure's cause from pixel input. This
                    inferred reason, along with the top-K most relevant error correction examples from memory, is then fed into the planner
                    to generate a corrective program.</li>
                    <li align="justify"><b>EXECUTOR</b>: translates each program step into specific navigation and manipulation actions. The EXECUTOR has the following sub-components:
                        <ol type="a">
                        <li align="justify"><b>Perception</b>: The Executor builds a semantic map using an off-the-shelf object detector and monocular depth estimation network. It detects and keeps track of object states (cooked, dirty, ...) via a vision-language model.</li>
                        <li align="justify"><b>Action execution</b>: It executes navigation code steps by shortest path planning in the semantic obstacle map and executes manipulation code steps
                        by calling on a set of low-level manipulation actions (pickup(X), slice(X), etc.).</li>
                        <li align="justify"><b>Pre-condition check</b>: Verifies if the necessary preconditions for an action, and the plan is adjusted according to the current environmental and agent state.</li>
                        <li align="justify"><b>LOCATOR</b>: The LOCATOR module efficiently searches for a required object by utilizing previous user instructions and LLMs' commonsense knowledge.</li>
                        </ol>
                    </li>
                    </ol>
                    </p>
                    <br>

                    <h3 align="justify">Memory-Augmented Prompting</h3>
                    <p align="justify">A key component of HELPER is its <b>memory of language-program pairs</b> to generate <b>tailored prompts
                        for pretrained LLMs based on the current
                        language context</b>. This aids in parsing
                        diverse, and user-specific linguistic inputs for
                        planning, re-planning during failures, and
                        interpreting human feedback.</p>
                    <img src="images/PlannerCombinedFigure_V5_katefversion2_combinedwfeedbackpersonal_V3.jpg" style="width:100%">
                    <br>
                    <br>
                    <br>
                    <h3 align="justify">Visually-Grounded Error Correction</h3>
                    <p align="justify">HELPER uses a <b>pretrained VLM to
                    obtain an action failure reason</b> and uses it to
                    retrieve similar failure cases with solutions for
                    memory-augmented LLM failure correction.</p>
                    <img src="images/Rectifier_Figure_V1.jpg" style="width:70%">
                    
                    <br>
                    <spacer type="horizontal" width="100" height="50"> </spacer>
                    <br>
                    <hr>
                    <h1>Results</h1>
                    <div class="w3-center">
                        <video width="100%" controls playsinline autoplay muted loop>
                            <source src="images/Teaser_video_selfplay_highdef.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <h2>Household Task Execution from Messy Dialogue</h2>
                    <p align="justify">We set a new <b>state-of-the-art in the TEACh benchmark</b>, where the agent is given a messy dialogue segment at the
                    start of the episode and an RGB sensor and is tasked to
                    infer the sequence of actions to execute based on
                    the user’s intents in the dialogue segment, ranging
                    from MAKE COFFEE to PREPARE BREAKFAST. <b>HELPER improves
                    task success by 1.7x and goal-condition success by 2.1x over existing works with minimal
                    in-domain finetuning.</b></p>
                    <br>
                    <h3 align="justify">Make a salad demo with module visualizations</h3>
                    <div class="w3-center">
                        <video width="90%" controls playsinline muted>
                            <source src="images/Full Task video with task text.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <br>
                    <br>
                    <h3 align="justify">Error correction demo with module visualizations</h3>
                    <div class="w3-center">
                        <video width="90%" controls playsinline muted>
                            <source src="images/Failure_video_HELPER2.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <br>
                    <br>
                    <!-- <h3 align="justify">Task Videos</h3> -->
                    
                    <!-- <h3 align="justify">TODO: ADD MORE VIDEOS & TASKS HERE</h3> -->
                    <br>
                    <spacer type="horizontal" width="100" height="50"> </spacer>
                    <br>
                    <hr>
                    <h2>User Personalization</h2>
                    <p align="justify">HELPER expands its
                    memory of programs with successful executions of user specific procedures; it then
                    recalls them and adapt them in future interactions with the user, allowing for user-personalized references.</p>
                    <!-- <img src="images/user_personal_feedback_V2.jpg" style="width:70%"> -->
                    <p align="justify">We test HELPER on its ability retrieve a personalized routines and adapt them to accomodate zero,
                        one, two, or three changes (10 each) from the original plan. <b>Of the 40 inputs tested, HELPER successfully
                            recalled and adapted all but three personalized routines.</b></p>
                    <p align="justify">An example sample from the evaluation is shown below of the original instruction used to generate the personalized program and add it to memory, evaluation instructions for zero, one, two, and three requested changes of
                        the routine.</p>
                    <p align="justify"><b>Original Instruction:</b> <font color="#9900FF">"Make me a salad. The name of this salad is called the David salad. The
                    salad has two slices of tomato and three slices of lettuce on a clean plate."</font></p>
                    
                    <p align="justify"><b>No Change Instruction:</b> <font color="#9900FF">"Make me the David salad"</font></p>
                    
                    <p align="justify"><b>One Change Instruction:</b> <font color="#9900FF">"Make me the David salad with a slice of potato"</font></p>
                    
                    <p align="justify"><b>Two Change Instruction:</b> <font color="#9900FF">"Make me the David salad but add a slice of potato and add one slice of lettuce"</font></p>
                    
                    <p align="justify"><b>Three Change Instruction:</b> <font color="#9900FF">"Make me the David salad and add a slice of potato, add one slice of lettuce, and bring a fork with it"</font></p>
                    <br>
                    <spacer type="horizontal" width="100" height="50"> </spacer>
                    <br>
                    <hr>
                    <h2>User Feedback</h2>
                    <p align="justify">Gathering user feedback can improve a home
                    robot’s performance, but frequently requesting
                    feedback on a task can diminish the overall user experience. Thus, we enable HELPER to elicit sparse
                    user feedback only when it has completed execution of the program from the initial user input. <b>HELPER improves an additional 1.3X in task success when incorporating just two user feedbacks.</b></p>
                    <img src="images/User_Feedback.jpg" style="width:70%">
                    <br>
                    <br>
                    <p align="justify"><b>VIDEO:</b> Clean all cookware with user feedback (skip to 0:41 for user feedback saying HELPER missed cleaning the pot)</p>
                    <video id="home2" width="50%" poster="images/video.jpg" controls="controls" preload="none">
                        <source type="video/mp4" src="images/task_videos/4f4013ceab294a80_3864.tfd_4f4013ceab294a80_3864_145_e31f2cd6a8b6545446fa.mp4" />
                    </video>
                    <br>
                    <br>
                    <p align="justify"><b>VIDEO:</b> Make breakfast with user feedback (skip to 2:54 for user feedback saying did not put tomato & lettuce slice on plate)</p>
                    <video id="home2" width="50%" poster="images/video.jpg" controls="controls" preload="none">
                        <source type="video/mp4"
                            src="images/task_videos/c66573f82df8618c_aad3.tfd_c66573f82df8618c_aad3_80_b5980228dfd5fc4d1ce9.mp4" />
                    </video>
                    <br>
                    <!-- <p> </p> -->

                    <!-- <p align="justify"><b>A. fMRI Response-Optimized Training Procedure.</b> All voxels in an ROI share a convolutional backbone. Each voxel is assigned a set of learnable spatial and feature weights. The voxel weights are applied to the output features of the backbone network to predict the voxel response for an input image. Training minimizes the mean squared error between the predicted and actual voxel response of NSD images.</p>
                    <p align="justify"><b>B. Dissection Procedure.</b> For each voxel in an ROI and test image, we obtain the voxel's selectivity binary mask by applying the voxel's feature weights to the output of the ROI-specific backbone and thresholding the resultant feature map. We examine the overlapping region between the mask and a measurement of interest (e.g., depth map).</p> -->
                </div>
                <br>
                <spacer type="horizontal" width="100" height="50"> </spacer>
                <br>
                <!-- <hr> -->
                
                <!-- <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <h2 id="results">Visualizations of </h2>
                </div> -->


                <!-- <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <h2 id="results">Results</h2>
                </div> -->                

                <br>
                <spacer type="horizontal" width="100" height="50"> </spacer>
                <br>
                <hr>
                <h4>See our paper for more!</h4>


                <section id="citation">
                    <h2>Citation</h2>
                    <div class="cit_cont">
                        <pre style="background-color:#F5F5F5;"><code><font size="-1">@proceedings{findings-2023-findings-association-linguistics-emnlp,
                        title = "Findings of the Association for Computational Linguistics: EMNLP 2023",
                        editor = "Sarch, Gabriel and
                        Wu, Yue and
                        Tarr, Michael and
                        Fragkiadaki, Katerina",
                        month = dec,
                        year = "2023",
                        publisher = "Association for Computational Linguistics",
                        }</font></code></pre>
                    </div>
                </section>
                <hr>
                <br>
                <br>

                <!-- end paper container -->

            </div><!-- End Grid -->
        </div><!-- End Page Container -->

</body>

</html>